{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bd8764a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\binur\\anaconda3\\envs\\tfradeon\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Importing the necessary libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac7356bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the data\n",
    "inputs = np.array([[73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70], \n",
    "                   [74, 66, 43], \n",
    "                   [91, 87, 65], \n",
    "                   [88, 134, 59], \n",
    "                   [101, 44, 37], \n",
    "                   [68, 96, 71], \n",
    "                   [73, 66, 44], \n",
    "                   [92, 87, 64], \n",
    "                   [87, 135, 57], \n",
    "                   [103, 43, 36], \n",
    "                   [68, 97, 70]],\n",
    "                   dtype='float32')\n",
    "\n",
    "targets = np.array([[56, 70], \n",
    "                    [81, 101], \n",
    "                    [119, 133], \n",
    "                    [22, 37], \n",
    "                    [103, 119],\n",
    "                    [57, 69], \n",
    "                    [80, 102], \n",
    "                    [118, 132], \n",
    "                    [21, 38], \n",
    "                    [104, 118], \n",
    "                    [57, 69], \n",
    "                    [82, 100], \n",
    "                    [118, 134], \n",
    "                    [20, 38], \n",
    "                    [102, 120]], \n",
    "                   dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3224f8",
   "metadata": {},
   "source": [
    "* Input and target matrices are loaded as NumPy arrays which is should be converted to torch tensors using the torch.from_numpy() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c94f39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16c383c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 73.,  67.,  43.],\n",
      "        [ 91.,  88.,  64.],\n",
      "        [ 87., 134.,  58.],\n",
      "        [102.,  43.,  37.],\n",
      "        [ 69.,  96.,  70.],\n",
      "        [ 74.,  66.,  43.],\n",
      "        [ 91.,  87.,  65.],\n",
      "        [ 88., 134.,  59.],\n",
      "        [101.,  44.,  37.],\n",
      "        [ 68.,  96.,  71.],\n",
      "        [ 73.,  66.,  44.],\n",
      "        [ 92.,  87.,  64.],\n",
      "        [ 87., 135.,  57.],\n",
      "        [103.,  43.,  36.],\n",
      "        [ 68.,  97.,  70.]])\n"
     ]
    }
   ],
   "source": [
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c733088a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.],\n",
      "        [ 57.,  69.],\n",
      "        [ 80., 102.],\n",
      "        [118., 132.],\n",
      "        [ 21.,  38.],\n",
      "        [104., 118.],\n",
      "        [ 57.,  69.],\n",
      "        [ 82., 100.],\n",
      "        [118., 134.],\n",
      "        [ 20.,  38.],\n",
      "        [102., 120.]])\n"
     ]
    }
   ],
   "source": [
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53aac93a",
   "metadata": {},
   "source": [
    "* **TensorDataset** wraps inputs and targets tensors into a single dataset. We can access rows from the dataset as tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac3b1927",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(inputs,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "006fade5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 73.,  67.,  43.],\n",
       "         [ 91.,  88.,  64.],\n",
       "         [ 87., 134.,  58.]]),\n",
       " tensor([[ 56.,  70.],\n",
       "         [ 81., 101.],\n",
       "         [119., 133.]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d57c393",
   "metadata": {},
   "source": [
    "* Using **DataLoader** we can convert the dataset into batches of predefined batch size and create batches by picking samples from the dataset randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61d08789",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "train_loader = DataLoader(dataset,batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb476a28",
   "metadata": {},
   "source": [
    "* We can access the data from DataLoader as a tuple pair containing input and corresponding targets using a for loop which enables us to load batches directly into a training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85dc312a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[101.,  44.,  37.],\n",
      "        [ 87., 135.,  57.],\n",
      "        [ 92.,  87.,  64.]])\n",
      "tensor([[ 21.,  38.],\n",
      "        [118., 134.],\n",
      "        [ 82., 100.]])\n"
     ]
    }
   ],
   "source": [
    "for inp, target in train_loader:\n",
    "    print(inp)\n",
    "    print(target)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf51330",
   "metadata": {},
   "source": [
    "* **torch.randn** generates tensors randomly from a uniform distribution with mean 0 and standard deviation 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de3afb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4490, -0.1964,  0.4375],\n",
      "        [-0.6483,  0.3561,  1.0225]], requires_grad=True)\n",
      "tensor([ 0.9262, -0.4477], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "w = torch.randn(2,3,requires_grad=True)\n",
    "b = torch.randn(2,requires_grad=True)\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a71aeb",
   "metadata": {},
   "source": [
    "* The model is just a mathematical equation establishing a linear relationship between weights and outputs. Matrix multiplication is performed (**@ represents matrix multiplication**) with the input batch and the transpose of the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d1b2f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    return x @ w.t() + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fba070df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction is :n tensor([[-17.3975,  62.2501],\n",
      "        [-26.4495,  19.0485],\n",
      "        [-18.2839,  60.5793]], grad_fn=<AddBackward0>)\n",
      "Actual targets is : tensor([[104., 118.],\n",
      "        [ 57.,  69.],\n",
      "        [103., 119.]])\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_loader:\n",
    "    preds = model(x)\n",
    "    print('Prediction is :n',preds)\n",
    "    print('Actual targets is :',y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65416cbe",
   "metadata": {},
   "source": [
    "* **numel()** method returns the number of elements in the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7b9bf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(predictions,targets):\n",
    "    difference = predictions - targets\n",
    "    return torch.sum(difference * difference) / difference.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12449465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction is : tensor([[-18.2839,  60.5793],\n",
      "        [-18.0313,  61.5837],\n",
      "        [-29.2159,  37.3337]], grad_fn=<AddBackward0>)\n",
      "Actual target is : tensor([[103., 119.],\n",
      "        [102., 120.],\n",
      "        [ 81., 101.]])\n",
      "Loss is: tensor(8690.6143, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_loader:\n",
    "    preds = model(x)\n",
    "    print('Prediction is :',preds)\n",
    "    print('Actual target is :',y)\n",
    "    print('Loss is:',mse_loss(preds,y))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3934db02",
   "metadata": {},
   "source": [
    "* Gradient descent is an optimization algorithm that calculates the derivative/gradient of the loss function to update the weights and correspondingly reduce the loss or find the minima of the loss function.\n",
    "\n",
    "\n",
    "* Steps to implement Gradient Descent in PyTorch,\n",
    "1. First, calculate the loss function\n",
    "2. Find the Gradient of the loss with respect to independent variables\n",
    "3. Update the weights and bais\n",
    "4. Repeat the above step\n",
    "5. Now let’s get into coding and implement Gradient Descent for 50 epochs,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f39f391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/50: Loss: 9316.3896484375\n",
      "Epoch 0/50: Loss: 9681.3037109375\n",
      "Epoch 0/50: Loss: 6102.611328125\n",
      "Epoch 0/50: Loss: 3368.697021484375\n",
      "Epoch 0/50: Loss: 7756.03662109375\n",
      "Epoch 1/50: Loss: 5367.17431640625\n",
      "Epoch 1/50: Loss: 6045.43115234375\n",
      "Epoch 1/50: Loss: 10409.9111328125\n",
      "Epoch 1/50: Loss: 4623.92333984375\n",
      "Epoch 1/50: Loss: 3719.714111328125\n",
      "Epoch 2/50: Loss: 5218.77392578125\n",
      "Epoch 2/50: Loss: 6909.79296875\n",
      "Epoch 2/50: Loss: 4424.78759765625\n",
      "Epoch 2/50: Loss: 4729.55322265625\n",
      "Epoch 2/50: Loss: 3978.628173828125\n",
      "Epoch 3/50: Loss: 5384.90185546875\n",
      "Epoch 3/50: Loss: 5436.982421875\n",
      "Epoch 3/50: Loss: 3341.990966796875\n",
      "Epoch 3/50: Loss: 5664.88134765625\n",
      "Epoch 3/50: Loss: 1401.2091064453125\n",
      "Epoch 4/50: Loss: 1955.2626953125\n",
      "Epoch 4/50: Loss: 4124.0224609375\n",
      "Epoch 4/50: Loss: 2381.63623046875\n",
      "Epoch 4/50: Loss: 4463.767578125\n",
      "Epoch 4/50: Loss: 4902.69677734375\n",
      "Epoch 5/50: Loss: 1761.9635009765625\n",
      "Epoch 5/50: Loss: 4095.253173828125\n",
      "Epoch 5/50: Loss: 3874.088623046875\n",
      "Epoch 5/50: Loss: 1636.6710205078125\n",
      "Epoch 5/50: Loss: 3621.931640625\n",
      "Epoch 6/50: Loss: 5968.41552734375\n",
      "Epoch 6/50: Loss: 2467.660888671875\n",
      "Epoch 6/50: Loss: 1003.0651245117188\n",
      "Epoch 6/50: Loss: 2006.1588134765625\n",
      "Epoch 6/50: Loss: 1222.8748779296875\n",
      "Epoch 7/50: Loss: 1916.5013427734375\n",
      "Epoch 7/50: Loss: 660.2628784179688\n",
      "Epoch 7/50: Loss: 2500.746337890625\n",
      "Epoch 7/50: Loss: 2921.564453125\n",
      "Epoch 7/50: Loss: 2657.307373046875\n",
      "Epoch 8/50: Loss: 3684.737060546875\n",
      "Epoch 8/50: Loss: 1147.576171875\n",
      "Epoch 8/50: Loss: 2562.050537109375\n",
      "Epoch 8/50: Loss: 1033.8614501953125\n",
      "Epoch 8/50: Loss: 597.5139770507812\n",
      "Epoch 9/50: Loss: 2008.6036376953125\n",
      "Epoch 9/50: Loss: 2286.971435546875\n",
      "Epoch 9/50: Loss: 2127.103759765625\n",
      "Epoch 9/50: Loss: 399.7218322753906\n",
      "Epoch 9/50: Loss: 819.8692016601562\n",
      "Epoch 10/50: Loss: 1896.4921875\n",
      "Epoch 10/50: Loss: 1470.56640625\n",
      "Epoch 10/50: Loss: 559.2185668945312\n",
      "Epoch 10/50: Loss: 886.8161010742188\n",
      "Epoch 10/50: Loss: 1669.7010498046875\n",
      "Epoch 11/50: Loss: 529.9492797851562\n",
      "Epoch 11/50: Loss: 1444.3125\n",
      "Epoch 11/50: Loss: 1270.2142333984375\n",
      "Epoch 11/50: Loss: 1233.1527099609375\n",
      "Epoch 11/50: Loss: 1041.7767333984375\n",
      "Epoch 12/50: Loss: 591.74951171875\n",
      "Epoch 12/50: Loss: 1030.9163818359375\n",
      "Epoch 12/50: Loss: 1210.78173828125\n",
      "Epoch 12/50: Loss: 915.0957641601562\n",
      "Epoch 12/50: Loss: 969.1614379882812\n",
      "Epoch 13/50: Loss: 204.4537811279297\n",
      "Epoch 13/50: Loss: 723.8843383789062\n",
      "Epoch 13/50: Loss: 1093.561279296875\n",
      "Epoch 13/50: Loss: 1132.475830078125\n",
      "Epoch 13/50: Loss: 899.99462890625\n",
      "Epoch 14/50: Loss: 1434.8681640625\n",
      "Epoch 14/50: Loss: 1014.109130859375\n",
      "Epoch 14/50: Loss: 434.87255859375\n",
      "Epoch 14/50: Loss: 215.77403259277344\n",
      "Epoch 14/50: Loss: 401.7117004394531\n",
      "Epoch 15/50: Loss: 236.52728271484375\n",
      "Epoch 15/50: Loss: 909.807373046875\n",
      "Epoch 15/50: Loss: 331.5724182128906\n",
      "Epoch 15/50: Loss: 448.2513122558594\n",
      "Epoch 15/50: Loss: 1103.0391845703125\n",
      "Epoch 16/50: Loss: 239.4479522705078\n",
      "Epoch 16/50: Loss: 360.9852600097656\n",
      "Epoch 16/50: Loss: 140.93443298339844\n",
      "Epoch 16/50: Loss: 726.4998168945312\n",
      "Epoch 16/50: Loss: 1175.7486572265625\n",
      "Epoch 17/50: Loss: 174.5798797607422\n",
      "Epoch 17/50: Loss: 694.531005859375\n",
      "Epoch 17/50: Loss: 680.9234008789062\n",
      "Epoch 17/50: Loss: 241.06304931640625\n",
      "Epoch 17/50: Loss: 517.1008911132812\n",
      "Epoch 18/50: Loss: 464.1983337402344\n",
      "Epoch 18/50: Loss: 361.1936340332031\n",
      "Epoch 18/50: Loss: 486.5248718261719\n",
      "Epoch 18/50: Loss: 145.54100036621094\n",
      "Epoch 18/50: Loss: 573.58154296875\n",
      "Epoch 19/50: Loss: 214.14866638183594\n",
      "Epoch 19/50: Loss: 471.7187194824219\n",
      "Epoch 19/50: Loss: 490.0469665527344\n",
      "Epoch 19/50: Loss: 115.88005828857422\n",
      "Epoch 19/50: Loss: 515.6295776367188\n",
      "Epoch 20/50: Loss: 419.6930847167969\n",
      "Epoch 20/50: Loss: 746.5994262695312\n",
      "Epoch 20/50: Loss: 188.35743713378906\n",
      "Epoch 20/50: Loss: 93.4906005859375\n",
      "Epoch 20/50: Loss: 172.3220672607422\n",
      "Epoch 21/50: Loss: 112.79302978515625\n",
      "Epoch 21/50: Loss: 437.3363037109375\n",
      "Epoch 21/50: Loss: 215.9220733642578\n",
      "Epoch 21/50: Loss: 376.0877685546875\n",
      "Epoch 21/50: Loss: 312.4670104980469\n",
      "Epoch 22/50: Loss: 348.5777587890625\n",
      "Epoch 22/50: Loss: 332.8919372558594\n",
      "Epoch 22/50: Loss: 378.9315490722656\n",
      "Epoch 22/50: Loss: 51.97666549682617\n",
      "Epoch 22/50: Loss: 210.5619354248047\n",
      "Epoch 23/50: Loss: 202.05638122558594\n",
      "Epoch 23/50: Loss: 276.1770324707031\n",
      "Epoch 23/50: Loss: 304.6756896972656\n",
      "Epoch 23/50: Loss: 113.23125457763672\n",
      "Epoch 23/50: Loss: 312.8077087402344\n",
      "Epoch 24/50: Loss: 309.2327575683594\n",
      "Epoch 24/50: Loss: 133.4795684814453\n",
      "Epoch 24/50: Loss: 314.5037536621094\n",
      "Epoch 24/50: Loss: 89.61410522460938\n",
      "Epoch 24/50: Loss: 270.3247375488281\n",
      "Epoch 25/50: Loss: 340.5059509277344\n",
      "Epoch 25/50: Loss: 88.2332534790039\n",
      "Epoch 25/50: Loss: 285.8752746582031\n",
      "Epoch 25/50: Loss: 193.60621643066406\n",
      "Epoch 25/50: Loss: 126.9114990234375\n",
      "Epoch 26/50: Loss: 118.01775360107422\n",
      "Epoch 26/50: Loss: 335.0729675292969\n",
      "Epoch 26/50: Loss: 258.4339904785156\n",
      "Epoch 26/50: Loss: 242.42242431640625\n",
      "Epoch 26/50: Loss: 13.013832092285156\n",
      "Epoch 27/50: Loss: 250.578125\n",
      "Epoch 27/50: Loss: 235.0361328125\n",
      "Epoch 27/50: Loss: 59.19893264770508\n",
      "Epoch 27/50: Loss: 137.18202209472656\n",
      "Epoch 27/50: Loss: 228.17176818847656\n",
      "Epoch 28/50: Loss: 229.4634552001953\n",
      "Epoch 28/50: Loss: 339.0473327636719\n",
      "Epoch 28/50: Loss: 47.005489349365234\n",
      "Epoch 28/50: Loss: 82.71995544433594\n",
      "Epoch 28/50: Loss: 167.3119659423828\n",
      "Epoch 29/50: Loss: 245.03395080566406\n",
      "Epoch 29/50: Loss: 182.5770263671875\n",
      "Epoch 29/50: Loss: 129.93441772460938\n",
      "Epoch 29/50: Loss: 75.05982971191406\n",
      "Epoch 29/50: Loss: 191.13694763183594\n",
      "Epoch 30/50: Loss: 50.14055252075195\n",
      "Epoch 30/50: Loss: 275.5201416015625\n",
      "Epoch 30/50: Loss: 109.33196258544922\n",
      "Epoch 30/50: Loss: 267.0163269042969\n",
      "Epoch 30/50: Loss: 88.32257080078125\n",
      "Epoch 31/50: Loss: 132.1063995361328\n",
      "Epoch 31/50: Loss: 277.2128601074219\n",
      "Epoch 31/50: Loss: 35.371116638183594\n",
      "Epoch 31/50: Loss: 177.99278259277344\n",
      "Epoch 31/50: Loss: 135.5429229736328\n",
      "Epoch 32/50: Loss: 113.7325439453125\n",
      "Epoch 32/50: Loss: 216.9688262939453\n",
      "Epoch 32/50: Loss: 120.44427490234375\n",
      "Epoch 32/50: Loss: 163.85023498535156\n",
      "Epoch 32/50: Loss: 117.83165740966797\n",
      "Epoch 33/50: Loss: 2.2138986587524414\n",
      "Epoch 33/50: Loss: 127.59300994873047\n",
      "Epoch 33/50: Loss: 257.4278869628906\n",
      "Epoch 33/50: Loss: 114.26578521728516\n",
      "Epoch 33/50: Loss: 212.63356018066406\n",
      "Epoch 34/50: Loss: 211.58970642089844\n",
      "Epoch 34/50: Loss: 36.95298767089844\n",
      "Epoch 34/50: Loss: 83.12313842773438\n",
      "Epoch 34/50: Loss: 142.71653747558594\n",
      "Epoch 34/50: Loss: 218.46592712402344\n",
      "Epoch 35/50: Loss: 124.59524536132812\n",
      "Epoch 35/50: Loss: 129.67181396484375\n",
      "Epoch 35/50: Loss: 176.07469177246094\n",
      "Epoch 35/50: Loss: 1.2406355142593384\n",
      "Epoch 35/50: Loss: 247.51756286621094\n",
      "Epoch 36/50: Loss: 56.258453369140625\n",
      "Epoch 36/50: Loss: 194.7504119873047\n",
      "Epoch 36/50: Loss: 197.50550842285156\n",
      "Epoch 36/50: Loss: 104.34239959716797\n",
      "Epoch 36/50: Loss: 113.02783966064453\n",
      "Epoch 37/50: Loss: 134.41781616210938\n",
      "Epoch 37/50: Loss: 182.10211181640625\n",
      "Epoch 37/50: Loss: 91.43439483642578\n",
      "Epoch 37/50: Loss: 124.58173370361328\n",
      "Epoch 37/50: Loss: 120.42571258544922\n",
      "Epoch 38/50: Loss: 100.60791015625\n",
      "Epoch 38/50: Loss: 177.7859649658203\n",
      "Epoch 38/50: Loss: 153.52667236328125\n",
      "Epoch 38/50: Loss: 19.78742027282715\n",
      "Epoch 38/50: Loss: 190.6352081298828\n",
      "Epoch 39/50: Loss: 124.83853912353516\n",
      "Epoch 39/50: Loss: 43.157894134521484\n",
      "Epoch 39/50: Loss: 92.67768096923828\n",
      "Epoch 39/50: Loss: 177.4464569091797\n",
      "Epoch 39/50: Loss: 193.7509002685547\n",
      "Epoch 40/50: Loss: 115.45178985595703\n",
      "Epoch 40/50: Loss: 92.02930450439453\n",
      "Epoch 40/50: Loss: 192.01060485839844\n",
      "Epoch 40/50: Loss: 193.72039794921875\n",
      "Epoch 40/50: Loss: 30.29706382751465\n",
      "Epoch 41/50: Loss: 52.355960845947266\n",
      "Epoch 41/50: Loss: 176.8654022216797\n",
      "Epoch 41/50: Loss: 121.68592071533203\n",
      "Epoch 41/50: Loss: 92.27423858642578\n",
      "Epoch 41/50: Loss: 172.54600524902344\n",
      "Epoch 42/50: Loss: 121.82300567626953\n",
      "Epoch 42/50: Loss: 73.20506286621094\n",
      "Epoch 42/50: Loss: 200.8687744140625\n",
      "Epoch 42/50: Loss: 24.798898696899414\n",
      "Epoch 42/50: Loss: 185.13348388671875\n",
      "Epoch 43/50: Loss: 187.33221435546875\n",
      "Epoch 43/50: Loss: 85.15299224853516\n",
      "Epoch 43/50: Loss: 124.2957534790039\n",
      "Epoch 43/50: Loss: 173.13612365722656\n",
      "Epoch 43/50: Loss: 29.63092041015625\n",
      "Epoch 44/50: Loss: 93.79012298583984\n",
      "Epoch 44/50: Loss: 121.28948211669922\n",
      "Epoch 44/50: Loss: 170.13710021972656\n",
      "Epoch 44/50: Loss: 104.31597900390625\n",
      "Epoch 44/50: Loss: 109.07134246826172\n",
      "Epoch 45/50: Loss: 143.3491668701172\n",
      "Epoch 45/50: Loss: 215.8029022216797\n",
      "Epoch 45/50: Loss: 20.33085060119629\n",
      "Epoch 45/50: Loss: 129.6962432861328\n",
      "Epoch 45/50: Loss: 81.39916229248047\n",
      "Epoch 46/50: Loss: 79.88147735595703\n",
      "Epoch 46/50: Loss: 183.2585906982422\n",
      "Epoch 46/50: Loss: 98.5902328491211\n",
      "Epoch 46/50: Loss: 124.94147491455078\n",
      "Epoch 46/50: Loss: 97.9491195678711\n",
      "Epoch 47/50: Loss: 102.13196563720703\n",
      "Epoch 47/50: Loss: 180.9459686279297\n",
      "Epoch 47/50: Loss: 17.749074935913086\n",
      "Epoch 47/50: Loss: 200.85565185546875\n",
      "Epoch 47/50: Loss: 78.45654296875\n",
      "Epoch 48/50: Loss: 188.0083770751953\n",
      "Epoch 48/50: Loss: 165.1380157470703\n",
      "Epoch 48/50: Loss: 177.66944885253906\n",
      "Epoch 48/50: Loss: 34.286590576171875\n",
      "Epoch 48/50: Loss: 7.976964950561523\n",
      "Epoch 49/50: Loss: 152.105712890625\n",
      "Epoch 49/50: Loss: 107.87722778320312\n",
      "Epoch 49/50: Loss: 175.60919189453125\n",
      "Epoch 49/50: Loss: 22.216445922851562\n",
      "Epoch 49/50: Loss: 112.87000274658203\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "for i in range(epochs):\n",
    "    #Iterate through training dataloader\n",
    "    for x,y in train_loader:\n",
    "        #Generate prediction\n",
    "        preds = model(x)\n",
    "        \n",
    "        #Gets the loss and perform backpropagation\n",
    "        loss = mse_loss(preds,y)\n",
    "        loss.backward()\n",
    "        \n",
    "        #Lets update the weights\n",
    "        with torch.no_grad():\n",
    "            w -= w.grad * 1e-6\n",
    "            b -= b.grad * 1e-6\n",
    "            \n",
    "            #Set the gradients to zero\n",
    "            w.grad.zero_()\n",
    "            b.grad.zero_()\n",
    "            \n",
    "        print(f'Epoch {i}/{epochs}: Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca0fbff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction is : tensor([[ 94.2435, 124.2491],\n",
      "        [ 45.4435,  35.8608],\n",
      "        [ 60.4863,  69.4806]], grad_fn=<AddBackward0>)\n",
      "Actual targets is : tensor([[104., 118.],\n",
      "        [ 22.,  37.],\n",
      "        [ 56.,  70.]])\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_loader:\n",
    "    preds = model(x)\n",
    "    print('Prediction is :',preds)\n",
    "    print('Actual targets is :',y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9669d266",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
