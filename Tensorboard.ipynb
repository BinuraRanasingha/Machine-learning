{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d12d2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adae6a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>benign_0__mal_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.45</td>\n",
       "      <td>15.70</td>\n",
       "      <td>82.57</td>\n",
       "      <td>477.1</td>\n",
       "      <td>0.12780</td>\n",
       "      <td>0.17000</td>\n",
       "      <td>0.15780</td>\n",
       "      <td>0.08089</td>\n",
       "      <td>0.2087</td>\n",
       "      <td>0.07613</td>\n",
       "      <td>...</td>\n",
       "      <td>23.75</td>\n",
       "      <td>103.40</td>\n",
       "      <td>741.6</td>\n",
       "      <td>0.1791</td>\n",
       "      <td>0.5249</td>\n",
       "      <td>0.5355</td>\n",
       "      <td>0.1741</td>\n",
       "      <td>0.3985</td>\n",
       "      <td>0.12440</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>18.25</td>\n",
       "      <td>19.98</td>\n",
       "      <td>119.60</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>0.09463</td>\n",
       "      <td>0.10900</td>\n",
       "      <td>0.11270</td>\n",
       "      <td>0.07400</td>\n",
       "      <td>0.1794</td>\n",
       "      <td>0.05742</td>\n",
       "      <td>...</td>\n",
       "      <td>27.66</td>\n",
       "      <td>153.20</td>\n",
       "      <td>1606.0</td>\n",
       "      <td>0.1442</td>\n",
       "      <td>0.2576</td>\n",
       "      <td>0.3784</td>\n",
       "      <td>0.1932</td>\n",
       "      <td>0.3063</td>\n",
       "      <td>0.08368</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13.71</td>\n",
       "      <td>20.83</td>\n",
       "      <td>90.20</td>\n",
       "      <td>577.9</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0.16450</td>\n",
       "      <td>0.09366</td>\n",
       "      <td>0.05985</td>\n",
       "      <td>0.2196</td>\n",
       "      <td>0.07451</td>\n",
       "      <td>...</td>\n",
       "      <td>28.14</td>\n",
       "      <td>110.60</td>\n",
       "      <td>897.0</td>\n",
       "      <td>0.1654</td>\n",
       "      <td>0.3682</td>\n",
       "      <td>0.2678</td>\n",
       "      <td>0.1556</td>\n",
       "      <td>0.3196</td>\n",
       "      <td>0.11510</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13.00</td>\n",
       "      <td>21.82</td>\n",
       "      <td>87.50</td>\n",
       "      <td>519.8</td>\n",
       "      <td>0.12730</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.18590</td>\n",
       "      <td>0.09353</td>\n",
       "      <td>0.2350</td>\n",
       "      <td>0.07389</td>\n",
       "      <td>...</td>\n",
       "      <td>30.73</td>\n",
       "      <td>106.20</td>\n",
       "      <td>739.3</td>\n",
       "      <td>0.1703</td>\n",
       "      <td>0.5401</td>\n",
       "      <td>0.5390</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.4378</td>\n",
       "      <td>0.10720</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12.46</td>\n",
       "      <td>24.04</td>\n",
       "      <td>83.97</td>\n",
       "      <td>475.9</td>\n",
       "      <td>0.11860</td>\n",
       "      <td>0.23960</td>\n",
       "      <td>0.22730</td>\n",
       "      <td>0.08543</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>0.08243</td>\n",
       "      <td>...</td>\n",
       "      <td>40.68</td>\n",
       "      <td>97.65</td>\n",
       "      <td>711.4</td>\n",
       "      <td>0.1853</td>\n",
       "      <td>1.0580</td>\n",
       "      <td>1.1050</td>\n",
       "      <td>0.2210</td>\n",
       "      <td>0.4366</td>\n",
       "      <td>0.20750</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "5        12.45         15.70           82.57      477.1          0.12780   \n",
       "6        18.25         19.98          119.60     1040.0          0.09463   \n",
       "7        13.71         20.83           90.20      577.9          0.11890   \n",
       "8        13.00         21.82           87.50      519.8          0.12730   \n",
       "9        12.46         24.04           83.97      475.9          0.11860   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760         0.30010              0.14710         0.2419   \n",
       "1           0.07864         0.08690              0.07017         0.1812   \n",
       "2           0.15990         0.19740              0.12790         0.2069   \n",
       "3           0.28390         0.24140              0.10520         0.2597   \n",
       "4           0.13280         0.19800              0.10430         0.1809   \n",
       "5           0.17000         0.15780              0.08089         0.2087   \n",
       "6           0.10900         0.11270              0.07400         0.1794   \n",
       "7           0.16450         0.09366              0.05985         0.2196   \n",
       "8           0.19320         0.18590              0.09353         0.2350   \n",
       "9           0.23960         0.22730              0.08543         0.2030   \n",
       "\n",
       "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871  ...          17.33           184.60      2019.0   \n",
       "1                 0.05667  ...          23.41           158.80      1956.0   \n",
       "2                 0.05999  ...          25.53           152.50      1709.0   \n",
       "3                 0.09744  ...          26.50            98.87       567.7   \n",
       "4                 0.05883  ...          16.67           152.20      1575.0   \n",
       "5                 0.07613  ...          23.75           103.40       741.6   \n",
       "6                 0.05742  ...          27.66           153.20      1606.0   \n",
       "7                 0.07451  ...          28.14           110.60       897.0   \n",
       "8                 0.07389  ...          30.73           106.20       739.3   \n",
       "9                 0.08243  ...          40.68            97.65       711.4   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "5            0.1791             0.5249           0.5355                0.1741   \n",
       "6            0.1442             0.2576           0.3784                0.1932   \n",
       "7            0.1654             0.3682           0.2678                0.1556   \n",
       "8            0.1703             0.5401           0.5390                0.2060   \n",
       "9            0.1853             1.0580           1.1050                0.2210   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  benign_0__mal_1  \n",
       "0          0.4601                  0.11890                0  \n",
       "1          0.2750                  0.08902                0  \n",
       "2          0.3613                  0.08758                0  \n",
       "3          0.6638                  0.17300                0  \n",
       "4          0.2364                  0.07678                0  \n",
       "5          0.3985                  0.12440                0  \n",
       "6          0.3063                  0.08368                0  \n",
       "7          0.3196                  0.11510                0  \n",
       "8          0.4378                  0.10720                0  \n",
       "9          0.4366                  0.20750                0  \n",
       "\n",
       "[10 rows x 31 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the dataset\n",
    "data = pd.read_csv('D:\\\\SLIIT\\\\3rd year 2nd sem\\\\Machine Learning amd Optimization Methods\\\\Coding\\\\cancer_classification.csv')\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "182ff0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.drop('benign_0__mal_1',axis=1).values\n",
    "y = data['benign_0__mal_1'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe1ff90a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n",
       "        1.189e-01],\n",
       "       [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n",
       "        8.902e-02],\n",
       "       [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n",
       "        8.758e-02],\n",
       "       ...,\n",
       "       [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n",
       "        7.820e-02],\n",
       "       [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n",
       "        1.240e-01],\n",
       "       [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n",
       "        7.039e-02]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03bce454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5430e5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the dataset into training and testing\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.5,random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5e38464",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling the data\n",
    "scaler = MinMaxScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba2e3c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss',mode='min',patience=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf6840f",
   "metadata": {},
   "source": [
    "The above code initializes an instance of the EarlyStopping callback function with three parameters:\n",
    "* **monitor='val_loss':** This specifies the quantity to be monitored during training, which is the validation loss in this case. The training process will monitor the validation loss and stop training if it stops decreasing.\n",
    "\n",
    "\n",
    "* **mode='min':** This specifies the direction of the monitored quantity that represents improvement. Since we want the validation loss to decrease during training, we set mode to 'min'.\n",
    "\n",
    "\n",
    "* **patience=25:** This specifies the number of epochs to wait before stopping training if the monitored quantity stops improving. In this case, if the validation loss stops decreasing for 25 consecutive epochs, training will be stopped early."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ac5a00",
   "metadata": {},
   "source": [
    "## Creating the TensorBoard Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa52678a",
   "metadata": {},
   "source": [
    "* TensorBoard is a visualization tool provided with TensorFlow.\n",
    "\n",
    "\n",
    "* This callback logs events for TensorBoard, including metrics summary plots,training graph visualization, activation histograms, and sampled profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0730948a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-02-28 :15:55:30'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now().strftime('%Y-%m-%d :%H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadeee20",
   "metadata": {},
   "source": [
    "* **datetime.now().strftime('%Y-%m-%d :%H:%M:%S')** is used to generate a timestamp in the format of YYYY-MM-DD :HH:MM:SS.\n",
    "\n",
    "\n",
    "* **datetime.now()** returns the current date and time as a datetime object.\n",
    "\n",
    "\n",
    "**strftime()** is a method of the datetime class that formats the datetime object into a string with the specified format.\n",
    "* %Y is a format code that represents the year with century as a decimal number.\n",
    "* %m is a format code that represents the month as a zero-padded decimal number.\n",
    "* %d is a format code that represents the day of the month as a zero-padded decimal number.\n",
    "* %H is a format code that represents the hour as a zero-padded decimal number using a 24-hour clock.\n",
    "* %M is a format code that represents the minute as a zero-padded decimal number.\n",
    "* %S is a format code that represents the second as a zero-padded decimal number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aaa7e071",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_directory = 'log\\\\fit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10997b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the Tensorboard\n",
    "board = TensorBoard(log_dir=log_directory,histogram_freq=1,write_graph=True,write_images=True,write_steps_per_second=False,\n",
    "                   update_freq='epcoh',profile_batch=2,embeddings_freq=1,embeddings_metadata=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7af1830e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(30,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(15,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "714909aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling the model\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "844ca346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "9/9 [==============================] - 2s 118ms/step - loss: 0.6768 - val_loss: 0.6731\n",
      "Epoch 2/600\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 0.6804 - val_loss: 0.6596\n",
      "Epoch 3/600\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.6713 - val_loss: 0.6473\n",
      "Epoch 4/600\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.6254 - val_loss: 0.6335\n",
      "Epoch 5/600\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.6317 - val_loss: 0.6130\n",
      "Epoch 6/600\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.6089 - val_loss: 0.5894\n",
      "Epoch 7/600\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.6009 - val_loss: 0.5665\n",
      "Epoch 8/600\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.5775 - val_loss: 0.5483\n",
      "Epoch 9/600\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.5549 - val_loss: 0.5313\n",
      "Epoch 10/600\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 0.5409 - val_loss: 0.5091\n",
      "Epoch 11/600\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.5339 - val_loss: 0.4889\n",
      "Epoch 12/600\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.5217 - val_loss: 0.4702\n",
      "Epoch 13/600\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.5052 - val_loss: 0.4519\n",
      "Epoch 14/600\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.4834 - val_loss: 0.4324\n",
      "Epoch 15/600\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.4415 - val_loss: 0.4130\n",
      "Epoch 16/600\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 0.4689 - val_loss: 0.3958\n",
      "Epoch 17/600\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.4261 - val_loss: 0.3825\n",
      "Epoch 18/600\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 0.3995 - val_loss: 0.3708\n",
      "Epoch 19/600\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.3996 - val_loss: 0.3586\n",
      "Epoch 20/600\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.3770 - val_loss: 0.3418\n",
      "Epoch 21/600\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 0.3911 - val_loss: 0.3282\n",
      "Epoch 22/600\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.3759 - val_loss: 0.3179\n",
      "Epoch 23/600\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.3587 - val_loss: 0.3083\n",
      "Epoch 24/600\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.3510 - val_loss: 0.2970\n",
      "Epoch 25/600\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 0.3516 - val_loss: 0.2892\n",
      "Epoch 26/600\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.3573 - val_loss: 0.2812\n",
      "Epoch 27/600\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.3318 - val_loss: 0.2770\n",
      "Epoch 28/600\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.3170 - val_loss: 0.2690\n",
      "Epoch 29/600\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.2988 - val_loss: 0.2625\n",
      "Epoch 30/600\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 0.2958 - val_loss: 0.2501\n",
      "Epoch 31/600\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 0.2898 - val_loss: 0.2423\n",
      "Epoch 32/600\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.2815 - val_loss: 0.2368\n",
      "Epoch 33/600\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 0.2735 - val_loss: 0.2319\n",
      "Epoch 34/600\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.2899 - val_loss: 0.2340\n",
      "Epoch 35/600\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.2885 - val_loss: 0.2256\n",
      "Epoch 36/600\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 0.2681 - val_loss: 0.2186\n",
      "Epoch 37/600\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.2490 - val_loss: 0.2162\n",
      "Epoch 38/600\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.2397 - val_loss: 0.2094\n",
      "Epoch 39/600\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.2174 - val_loss: 0.2080\n",
      "Epoch 40/600\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.2362 - val_loss: 0.2011\n",
      "Epoch 41/600\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2441 - val_loss: 0.1964\n",
      "Epoch 42/600\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 0.2235 - val_loss: 0.1937\n",
      "Epoch 43/600\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 0.2350 - val_loss: 0.1912\n",
      "Epoch 44/600\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2206 - val_loss: 0.1898\n",
      "Epoch 45/600\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.2232 - val_loss: 0.1867\n",
      "Epoch 46/600\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.2119 - val_loss: 0.1838\n",
      "Epoch 47/600\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2241 - val_loss: 0.1815\n",
      "Epoch 48/600\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 0.2349 - val_loss: 0.1795\n",
      "Epoch 49/600\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.1934 - val_loss: 0.1766\n",
      "Epoch 50/600\n",
      "9/9 [==============================] - 0s 47ms/step - loss: 0.2051 - val_loss: 0.1749\n",
      "Epoch 51/600\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.2017 - val_loss: 0.1717\n",
      "Epoch 52/600\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2111 - val_loss: 0.1707\n",
      "Epoch 53/600\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2366 - val_loss: 0.1668\n",
      "Epoch 54/600\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 0.1975 - val_loss: 0.1657\n",
      "Epoch 55/600\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.1831 - val_loss: 0.1621\n",
      "Epoch 56/600\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2214 - val_loss: 0.1614\n",
      "Epoch 57/600\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2057 - val_loss: 0.1580\n",
      "Epoch 58/600\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.1754 - val_loss: 0.1553\n",
      "Epoch 59/600\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1837 - val_loss: 0.1519\n",
      "Epoch 60/600\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.1722 - val_loss: 0.1503\n",
      "Epoch 61/600\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.1673 - val_loss: 0.1494\n",
      "Epoch 62/600\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.1865 - val_loss: 0.1495\n",
      "Epoch 63/600\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1697 - val_loss: 0.1487\n",
      "Epoch 64/600\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.1796 - val_loss: 0.1481\n",
      "Epoch 65/600\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1562 - val_loss: 0.1480\n",
      "Epoch 66/600\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1315 - val_loss: 0.1454\n",
      "Epoch 67/600\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.1851 - val_loss: 0.1406\n",
      "Epoch 68/600\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1459 - val_loss: 0.1389\n",
      "Epoch 69/600\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.1718 - val_loss: 0.1368\n",
      "Epoch 70/600\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.1494 - val_loss: 0.1350\n",
      "Epoch 71/600\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 0.1513 - val_loss: 0.1353\n",
      "Epoch 72/600\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 0.1509 - val_loss: 0.1344\n",
      "Epoch 73/600\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 0.1536 - val_loss: 0.1391\n",
      "Epoch 74/600\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1305 - val_loss: 0.1322\n",
      "Epoch 75/600\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 0.1500 - val_loss: 0.1301\n",
      "Epoch 76/600\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.1242 - val_loss: 0.1285\n",
      "Epoch 77/600\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1418 - val_loss: 0.1263\n",
      "Epoch 78/600\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.1356 - val_loss: 0.1251\n",
      "Epoch 79/600\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1135 - val_loss: 0.1243\n",
      "Epoch 80/600\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 0.1414 - val_loss: 0.1226\n",
      "Epoch 81/600\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 0.1213 - val_loss: 0.1211\n",
      "Epoch 82/600\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.1241 - val_loss: 0.1205\n",
      "Epoch 83/600\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 0.1291 - val_loss: 0.1199\n",
      "Epoch 84/600\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.1309 - val_loss: 0.1190\n",
      "Epoch 85/600\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1278 - val_loss: 0.1211\n",
      "Epoch 86/600\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.1353 - val_loss: 0.1207\n",
      "Epoch 87/600\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 0.1076 - val_loss: 0.1177\n",
      "Epoch 88/600\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.1149 - val_loss: 0.1165\n",
      "Epoch 89/600\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 0.1020 - val_loss: 0.1153\n",
      "Epoch 90/600\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 0.1301 - val_loss: 0.1148\n",
      "Epoch 91/600\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.1296 - val_loss: 0.1152\n",
      "Epoch 92/600\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.1346 - val_loss: 0.1191\n",
      "Epoch 93/600\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.1379 - val_loss: 0.1134\n",
      "Epoch 94/600\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.1431 - val_loss: 0.1130\n",
      "Epoch 95/600\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.1136 - val_loss: 0.1114\n",
      "Epoch 96/600\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.1120 - val_loss: 0.1103\n",
      "Epoch 97/600\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0925 - val_loss: 0.1097\n",
      "Epoch 98/600\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.1133 - val_loss: 0.1093\n",
      "Epoch 99/600\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 0.1371 - val_loss: 0.1100\n",
      "Epoch 100/600\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 0.1380 - val_loss: 0.1109\n",
      "Epoch 101/600\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 0.0812 - val_loss: 0.1121\n",
      "Epoch 102/600\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 0.0990 - val_loss: 0.1084\n",
      "Epoch 103/600\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 0.0923 - val_loss: 0.1074\n",
      "Epoch 104/600\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 0.1199 - val_loss: 0.1103\n",
      "Epoch 105/600\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 0.0998 - val_loss: 0.1081\n",
      "Epoch 106/600\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.1073 - val_loss: 0.1072\n",
      "Epoch 107/600\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0900 - val_loss: 0.1095\n",
      "Epoch 108/600\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.1365 - val_loss: 0.1058\n",
      "Epoch 109/600\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 0.0958 - val_loss: 0.1105\n",
      "Epoch 110/600\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.1021 - val_loss: 0.1086\n",
      "Epoch 111/600\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.1005 - val_loss: 0.1029\n",
      "Epoch 112/600\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.1070 - val_loss: 0.1024\n",
      "Epoch 113/600\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0866 - val_loss: 0.1021\n",
      "Epoch 114/600\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.1210 - val_loss: 0.1033\n",
      "Epoch 115/600\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0842 - val_loss: 0.1022\n",
      "Epoch 116/600\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0934 - val_loss: 0.1034\n",
      "Epoch 117/600\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0885 - val_loss: 0.1023\n",
      "Epoch 118/600\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0968 - val_loss: 0.1042\n",
      "Epoch 119/600\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.1124 - val_loss: 0.1031\n",
      "Epoch 120/600\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.1046 - val_loss: 0.1027\n",
      "Epoch 121/600\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0826 - val_loss: 0.1028\n",
      "Epoch 122/600\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0799 - val_loss: 0.1042\n",
      "Epoch 123/600\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.1074 - val_loss: 0.1009\n",
      "Epoch 124/600\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 0.1125 - val_loss: 0.0984\n",
      "Epoch 125/600\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0937 - val_loss: 0.0975\n",
      "Epoch 126/600\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0910 - val_loss: 0.0967\n",
      "Epoch 127/600\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0838 - val_loss: 0.0965\n",
      "Epoch 128/600\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0886 - val_loss: 0.0959\n",
      "Epoch 129/600\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0888 - val_loss: 0.0959\n",
      "Epoch 130/600\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 0.0984 - val_loss: 0.1017\n",
      "Epoch 131/600\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0955 - val_loss: 0.1013\n",
      "Epoch 132/600\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 0.0925 - val_loss: 0.0994\n",
      "Epoch 133/600\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0839 - val_loss: 0.0996\n",
      "Epoch 134/600\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 0.1080 - val_loss: 0.0943\n",
      "Epoch 135/600\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 0.1120 - val_loss: 0.0956\n",
      "Epoch 136/600\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0803 - val_loss: 0.0976\n",
      "Epoch 137/600\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0781 - val_loss: 0.0977\n",
      "Epoch 138/600\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0728 - val_loss: 0.0954\n",
      "Epoch 139/600\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0844 - val_loss: 0.0957\n",
      "Epoch 140/600\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 0.0999 - val_loss: 0.0960\n",
      "Epoch 141/600\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0977 - val_loss: 0.0970\n",
      "Epoch 142/600\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 0.0954 - val_loss: 0.1026\n",
      "Epoch 143/600\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 0.0853 - val_loss: 0.0999\n",
      "Epoch 144/600\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0666 - val_loss: 0.0970\n",
      "Epoch 145/600\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.1021 - val_loss: 0.0980\n",
      "Epoch 146/600\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 0.0700 - val_loss: 0.0966\n",
      "Epoch 147/600\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.0893 - val_loss: 0.0951\n",
      "Epoch 148/600\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0757 - val_loss: 0.0958\n",
      "Epoch 149/600\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0649 - val_loss: 0.1011\n",
      "Epoch 150/600\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 0.0877 - val_loss: 0.0942\n",
      "Epoch 151/600\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0853 - val_loss: 0.0947\n",
      "Epoch 152/600\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 0.0822 - val_loss: 0.0957\n",
      "Epoch 153/600\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0751 - val_loss: 0.0942\n",
      "Epoch 154/600\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0680 - val_loss: 0.0934\n",
      "Epoch 155/600\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0898 - val_loss: 0.0928\n",
      "Epoch 156/600\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0773 - val_loss: 0.0923\n",
      "Epoch 157/600\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0881 - val_loss: 0.0936\n",
      "Epoch 158/600\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 0.0761 - val_loss: 0.1068\n",
      "Epoch 159/600\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0805 - val_loss: 0.0952\n",
      "Epoch 160/600\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0788 - val_loss: 0.0933\n",
      "Epoch 161/600\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0747 - val_loss: 0.0963\n",
      "Epoch 162/600\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0771 - val_loss: 0.0942\n",
      "Epoch 163/600\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0808 - val_loss: 0.0945\n",
      "Epoch 164/600\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.1042 - val_loss: 0.0941\n",
      "Epoch 165/600\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0611 - val_loss: 0.0936\n",
      "Epoch 166/600\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0826 - val_loss: 0.0921\n",
      "Epoch 167/600\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0753 - val_loss: 0.0918\n",
      "Epoch 168/600\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0737 - val_loss: 0.0931\n",
      "Epoch 169/600\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 0.0781 - val_loss: 0.0930\n",
      "Epoch 170/600\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.0703 - val_loss: 0.0926\n",
      "Epoch 171/600\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.0688 - val_loss: 0.0954\n",
      "Epoch 172/600\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 0.0706 - val_loss: 0.0944\n",
      "Epoch 173/600\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 0.0686 - val_loss: 0.0928\n",
      "Epoch 174/600\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 0.0556 - val_loss: 0.0974\n",
      "Epoch 175/600\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.0554 - val_loss: 0.0976\n",
      "Epoch 176/600\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.0625 - val_loss: 0.0952\n",
      "Epoch 177/600\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.0836 - val_loss: 0.0966\n",
      "Epoch 178/600\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.0782 - val_loss: 0.0964\n",
      "Epoch 179/600\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 0.0437 - val_loss: 0.0958\n",
      "Epoch 180/600\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 0.0656 - val_loss: 0.0956\n",
      "Epoch 181/600\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0524 - val_loss: 0.0933\n",
      "Epoch 182/600\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 0.0883 - val_loss: 0.0952\n",
      "Epoch 183/600\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0806 - val_loss: 0.0936\n",
      "Epoch 184/600\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0777 - val_loss: 0.0942\n",
      "Epoch 185/600\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 0.0783 - val_loss: 0.0938\n",
      "Epoch 186/600\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0680 - val_loss: 0.0952\n",
      "Epoch 187/600\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0650 - val_loss: 0.0931\n",
      "Epoch 188/600\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0839 - val_loss: 0.0936\n",
      "Epoch 189/600\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.0556 - val_loss: 0.0935\n",
      "Epoch 190/600\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 0.0438 - val_loss: 0.0962\n",
      "Epoch 191/600\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0838 - val_loss: 0.0922\n",
      "Epoch 192/600\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0503 - val_loss: 0.0925\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28f7d345850>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training the model\n",
    "model.fit(x=x_train,y=y_train,epochs=600,validation_data=(x_test,y_test),verbose=1,callbacks=[early_stop,board])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d60dc08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
