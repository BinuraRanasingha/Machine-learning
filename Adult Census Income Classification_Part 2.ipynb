{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99b85518",
   "metadata": {},
   "source": [
    "* **Neural Networks**\n",
    "* **Ridge Classifier**\n",
    "* **PassiveAggressive Classifier**\n",
    "* **Support Vector Machines**\n",
    "* **NearestCentroid**\n",
    "* **KNN with NCA**\n",
    "* **Gaussian Process Classification**\n",
    "* **GausianNaiveBayes**\n",
    "* **Complement Naive Bayes**\n",
    "* **LinearSVC**\n",
    "* **NuSVC**\n",
    "* **MLPClassifier**\n",
    "* **LogisticRegression**\n",
    "* **HistGradientBoostingClassifier**\n",
    "* **CategoricalNB**\n",
    "\n",
    "## Ensemble methods\n",
    "### Boosting\n",
    "* **Gradient boosting algorithms (Gradient Boosting Machine, XGBoost, LightGBM, CatBoost)**\n",
    "* **Stacking**\n",
    "* **Adaboost**\n",
    "* **Gradient Boosted Regression Trees (GBRT)**\n",
    "* **LogitBoost**\n",
    "* **BrownBoost**\n",
    "\n",
    "### Bagging\n",
    "* **Random Forest**\n",
    "* **Bagged Decision Trees**\n",
    "* **Bagged Support Vector Machines**\n",
    "* **Bagged Neural Networks**\n",
    "* **Bagged k-Nearest Neighbors**\n",
    "* **Bagged Naive Bayes**\n",
    "* **Bagged linear regression**\n",
    "\n",
    "\n",
    "* **PyTorch**\n",
    "* **Theano**\n",
    "* Mxnet : Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
    "* **Lasange**\n",
    "* **NeuroLab**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7753e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the necessary libraries\n",
    "#Neural networks can be used in this case.\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, BaggingClassifier, ExtraTreesClassifier, AdaBoostClassifier,StackingClassifier, RandomForestRegressor, GradientBoostingRegressor, HistGradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso, ElasticNet, RidgeClassifier, SGDClassifier, PassiveAggressiveClassifier\n",
    "from sklearn.svm import SVC, SVR, LinearSVC, NuSVC\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB, CategoricalNB\n",
    "from logitboost import LogitBoost\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier, NeighborhoodComponentsAnalysis, NearestCentroid\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import neurolab as nl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de72300e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education.num</th>\n",
       "      <th>marital.status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital.gain</th>\n",
       "      <th>capital.loss</th>\n",
       "      <th>hours.per.week</th>\n",
       "      <th>native.country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77053</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82</td>\n",
       "      <td>Private</td>\n",
       "      <td>132870</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>18</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66</td>\n",
       "      <td>NaN</td>\n",
       "      <td>186061</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>Private</td>\n",
       "      <td>140359</td>\n",
       "      <td>7th-8th</td>\n",
       "      <td>4</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>3900</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>Private</td>\n",
       "      <td>264663</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>3900</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age workclass  fnlwgt     education  education.num marital.status  \\\n",
       "0   90       NaN   77053       HS-grad              9        Widowed   \n",
       "1   82   Private  132870       HS-grad              9        Widowed   \n",
       "2   66       NaN  186061  Some-college             10        Widowed   \n",
       "3   54   Private  140359       7th-8th              4       Divorced   \n",
       "4   41   Private  264663  Some-college             10      Separated   \n",
       "\n",
       "          occupation   relationship   race     sex  capital.gain  \\\n",
       "0                NaN  Not-in-family  White  Female             0   \n",
       "1    Exec-managerial  Not-in-family  White  Female             0   \n",
       "2                NaN      Unmarried  Black  Female             0   \n",
       "3  Machine-op-inspct      Unmarried  White  Female             0   \n",
       "4     Prof-specialty      Own-child  White  Female             0   \n",
       "\n",
       "   capital.loss  hours.per.week native.country income  \n",
       "0          4356              40  United-States  <=50K  \n",
       "1          4356              18  United-States  <=50K  \n",
       "2          4356              40  United-States  <=50K  \n",
       "3          3900              40  United-States  <=50K  \n",
       "4          3900              40  United-States  <=50K  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\User\\\\Desktop\\\\ML\\\\Adult Census Income Classification\\\\adult.csv\", na_values='?')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec578539",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4407956",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d419d7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [i for i in df.columns if df[i].dtype == 'O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4c4d24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in categorical_features:\n",
    "    df[i] = le.fit_transform(df[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ead5635e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education.num</th>\n",
       "      <th>occupation</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>hours.per.week</th>\n",
       "      <th>native.country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82</td>\n",
       "      <td>2</td>\n",
       "      <td>132870</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>2</td>\n",
       "      <td>140359</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>2</td>\n",
       "      <td>264663</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>34</td>\n",
       "      <td>2</td>\n",
       "      <td>216864</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>150601</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  workclass  fnlwgt  education  education.num  occupation  race  sex  \\\n",
       "1   82          2  132870         11              9           3     4    0   \n",
       "3   54          2  140359          5              4           6     4    0   \n",
       "4   41          2  264663         15             10           9     4    0   \n",
       "5   34          2  216864         11              9           7     4    0   \n",
       "6   38          2  150601          0              6           0     4    1   \n",
       "\n",
       "   hours.per.week  native.country  income  \n",
       "1              18              38       0  \n",
       "3              40              38       0  \n",
       "4              40              38       0  \n",
       "5              45              38       0  \n",
       "6              40              38       0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(['marital.status','relationship','capital.gain','capital.loss',],axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caab31c",
   "metadata": {},
   "source": [
    "x = df.drop(['income'],axis=1)\n",
    "y = df['income']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9918a198",
   "metadata": {},
   "source": [
    "#Dividing the dataset into training and testing\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.33, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14f63adf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'e' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43me\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'e' is not defined"
     ]
    }
   ],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207db14d",
   "metadata": {},
   "source": [
    "# KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbc042a",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268e09a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6520bd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = knn.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730235cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15820e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming X is your feature matrix\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(x)\n",
    "\n",
    "# Split the scaled data\n",
    "X_train_scaled, X_test_scaled, y_train1, y_test1 = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train k-NN classifier with scaled data\n",
    "knn_classifier_scaled = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_classifier_scaled.fit(X_train_scaled, y_train1)\n",
    "\n",
    "# Make predictions on the scaled test set\n",
    "predictions_scaled = knn_classifier_scaled.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_scaled = accuracy_score(y_test1, predictions_scaled)\n",
    "print(f'Accuracy with scaled data: {accuracy_scaled}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605d823f",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_param_grid = {\n",
    "    'n_neighbors' : [15,20,25],\n",
    "    'weights' : ['uniform','distance'],\n",
    "    'algorithm' : ['ball_tree','kd_tree','brute'],\n",
    "    'leaf_size' : [10,20,25],\n",
    "    'p' : [1,2,3]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8396608b",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_best = GridSearchCV(estimator=knn_classifier_scaled, param_grid=knn_param_grid, cv=5, verbose=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bea157d",
   "metadata": {},
   "source": [
    "knn_best.fit(X_train_scaled,y_train1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b405ad",
   "metadata": {},
   "source": [
    "knn_best.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c44874",
   "metadata": {},
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6c5cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(algorithm='ball_tree',leaf_size=10,n_neighbors=20,p=1,weights='uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f031cb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_train_scaled,y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d5ff77",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = knn.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8238827",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test1,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a96414",
   "metadata": {},
   "source": [
    "## KNN with NCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabfd198",
   "metadata": {},
   "source": [
    "* The **Neighbourhood Components Analysis (NCA)** is a method for learning a Mahalanobis distance metric for k-Nearest Neighbors (KNN) classification. It can be used to improve the performance of KNN by learning a linear transformation of the input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8131956a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline with NCA and KNN\n",
    "nca_knn_model = Pipeline([\n",
    "    ('nca', NeighborhoodComponentsAnalysis()),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b124a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the training data\n",
    "nca_knn_model.fit(X_train_scaled, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95f2542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "predictions = nca_knn_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6401b626",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test1,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d182a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_param_grid = {\n",
    "    'n_neighbors' : [15,20,25],\n",
    "    'weights' : ['uniform','distance'],\n",
    "    'algorithm' : ['ball_tree','kd_tree','brute'],\n",
    "    'leaf_size' : [10,20,25],\n",
    "    'p' : [1,2,3]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411a934a",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_best = GridSearchCV(estimator=knn_classifier_scaled, param_grid=knn_param_grid, cv=5, verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1550033",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_best.fit(X_train_scaled,y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59220d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_best.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e17f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline with NCA and KNN\n",
    "nca_knn_model = Pipeline([\n",
    "    ('nca', NeighborhoodComponentsAnalysis()),\n",
    "    ('knn', KNeighborsClassifier(algorithm='ball_tree',leaf_size=10,n_neighbors=20,p=1,weights='uniform'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768afd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the training data\n",
    "nca_knn_model.fit(X_train_scaled, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad5494b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "predictions = nca_knn_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82d19f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test1,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9887cd",
   "metadata": {},
   "source": [
    "# NearestCentroid Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834c90c6",
   "metadata": {},
   "source": [
    "* The **nearest centroid classifier** is a simple classification algorithm that assigns a test sample to the class whose centroid (mean feature vector) is closest to the sample's feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bd737f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Nearest Centroid classifier\n",
    "nearest_centroid_model = NearestCentroid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebef3349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the training data\n",
    "nearest_centroid_model.fit(X_train_scaled, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411b0d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "predictions = nearest_centroid_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b37fd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test1,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b6ce31",
   "metadata": {},
   "source": [
    "# RidgeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0a4baf",
   "metadata": {},
   "source": [
    "* The **Ridge Classifier** is a linear classifier that uses **Ridge regression** for binary and multiclass classification tasks. It's an extension of linear regression adapted for classification problems. The Ridge Classifier is particularly useful when there is multicollinearity in the feature matrix, meaning that some features are highly correlated.\n",
    "\n",
    "* **Regularization:** Like Ridge regression, the Ridge Classifier introduces regularization to prevent overfitting. Regularization adds a penalty term to the linear regression objective function, discouraging the model from fitting the training data too closely.\n",
    "\n",
    "* **Objective Function:** The objective function optimized by the Ridge Classifier is a combination of the mean squared loss (similar to linear regression) and a regularization term. The regularization term is controlled by a hyperparameter called alpha in scikit-learn.\n",
    "\n",
    "* **Decision Rule:** The decision rule of the Ridge Classifier involves comparing the signed distance of a data point to the decision boundary. The decision boundary is determined by the coefficients learned during the training process.\n",
    "\n",
    "* **Multiclass Classification:** For multiclass classification, Ridge Classifier uses a one-vs-one strategy, where it trains a binary classifier for each pair of classes and then combines their decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fcd0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Ridge Classifier\n",
    "ridge_classifier = RidgeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ae6cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the training data\n",
    "ridge_classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68887b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "predictions = ridge_classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8881760",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1838d8b",
   "metadata": {},
   "source": [
    "# SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657d76b8",
   "metadata": {},
   "source": [
    "* The **SGDClassifier** in scikit-learn is an implementation of a linear classifier using **Stochastic Gradient Descent(SGD) optimization**. It is suitable for large-scale machine learning problems and supports various loss functions and penalties for regularization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3764bebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the SGDClassifier\n",
    "sgd_classifier = SGDClassifier(loss='log_loss', max_iter=1000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe1d5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the training data\n",
    "sgd_classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a521cad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "predictions = sgd_classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053ec8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c19cc6",
   "metadata": {},
   "source": [
    "# LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cd2a89",
   "metadata": {},
   "source": [
    "* The **LinearSVC(Linear Support Vector Classification)** in scikit-learn is a linear classifier that uses Support Vector Machines (SVM) for classification tasks. It's similar to SVC (Support Vector Classification), but **LinearSVC uses a linear kernel**, which is well-suited for linearly separable datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc45bfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the LinearSVC\n",
    "linear_svc = LinearSVC(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa12d02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the training data\n",
    "linear_svc.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1590ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "predictions = linear_svc.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5511aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f6203b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVClin_param_grid = {\n",
    "    'penalty':['l1','l2'],\n",
    "    'loss':['hinge','squared_hinge'],\n",
    "    'dual' : [True,False],\n",
    "    'C' : [1,2,3,4],\n",
    "    'max_iter' : [10,20,30,40,50,60,70,80,90,100,150]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfefaf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_SVClin = GridSearchCV(estimator=linear_svc, param_grid=SVClin_param_grid, cv=5, verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ac12d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_SVClin.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3326b126",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_SVClin.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f072b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svc = LinearSVC(penalty='l1', dual=False, C=1, random_state=42, max_iter=750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5234a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svc.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7d23f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = linear_svc.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71f7094",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test,preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35531e7",
   "metadata": {},
   "source": [
    "# PassiveAggressiveClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11071d10",
   "metadata": {},
   "source": [
    "* The **PassiveAggressiveClassifier** in scikit-learn is a linear classification algorithm that is designed for online learning. It is particularly useful when dealing with large-scale and streaming datasets. The algorithm is named \"passive-aggressive\" because it updates its model parameters in a way that is both passive and aggressive:\n",
    "\n",
    "* Passive updates occur when the model correctly predicts the instance. In this case, the model doesn't change much.\n",
    "* Aggressive updates occur when the model makes a mistake. In this case, the model adjusts its parameters to correct the mistake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bce830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the PassiveAggressiveClassifier\n",
    "pa_classifier = PassiveAggressiveClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ca16cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the training data\n",
    "pa_classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3e77bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "predictions = pa_classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78f755f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c373cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pac_param_grid = {\n",
    "    'C' : [1,2,3],\n",
    "    'max_iter' : [500,600,700,800,900,1000,1100,1200],\n",
    "    'n_iter_no_change' : [1,2,3,4,5,6,7,8,9,10],\n",
    "    'shuffle' : [True,False],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b29e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pac_best = GridSearchCV(estimator=pa_classifier, param_grid=pac_param_grid, cv=5, verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafc0e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "pac_best.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699bdd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pac_best.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358af23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_classifier = PassiveAggressiveClassifier(random_state=42, C=1, max_iter=500, n_iter_no_change=7, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692a28c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_classifier.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e47f03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pa_classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876e9346",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bb654f",
   "metadata": {},
   "source": [
    "# NuSVC Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d34b203",
   "metadata": {},
   "source": [
    "* **NuSVC** is a variant of **Support Vector Classification(SVC)** in scikit-learn that introduces the hyperparameter nu. The **nu parameter is an upper bound on the fraction of margin errors and a lower bound of the fraction of support vectors**. It controls the trade-off between having a smooth decision boundary and classifying training points correctly.\n",
    "\n",
    "* **1. nu Parameter:** The nu parameter takes values in the interval (0, 1], representing an upper bound on the fraction of margin errors and a lower bound on the fraction of support vectors. A smaller value of nu leads to a softer margin, allowing for more training errors but potentially resulting in a more generalizable model. A larger value of nu enforces a stricter margin, reducing the number of allowable training errors but potentially making the model less sensitive to noise in the data.\n",
    "\n",
    "* **2. Similarity to C Parameter:** nu can be thought of as analogous to the C parameter in the standard SVC. However, nu has a different interpretation, and the optimal value for nu might not directly correspond to the optimal value for C.\n",
    "\n",
    "* **3. Flexibility:** NuSVC provides a flexible approach for controlling the balance between training error and margin size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630591ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "nusvc_param_grid = {\n",
    "    'kernel' : ['linear','rbf','sigmoid'],\n",
    "    'shrinking' : [True,False],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9783f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "nusvc = NuSVC(random_state=42,nu=0.06,kernel='poly',degree=3,gamma='scale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc82d0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "nusvc.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95384734",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = nusvc.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa74cf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aeeef7b",
   "metadata": {},
   "source": [
    "# HistGradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3128237",
   "metadata": {},
   "source": [
    "* **HistGradientBoostingClassifier** is a **gradient boosting algorithm** introduced in scikit-learn that is designed to handle large datasets more efficiently. It is part of the histogram-based gradient boosting family, which constructs histograms of the features to speed up the training process. This classifier is suitable for both binary and multiclass classification problems.\n",
    "\n",
    "Here are some key features and characteristics of HistGradientBoostingClassifier:\n",
    "* **1. Histogram-Based Approach:** Unlike traditional gradient boosting algorithms that use exact computation of gradients and splits, the histogram-based approach of HistGradientBoostingClassifier builds histograms of feature values to make the training process more efficient, especially for large datasets.\n",
    "\n",
    "* **2. Memory Efficiency:** By working with histograms, the algorithm reduces the memory requirements compared to exact algorithms. This makes it particularly useful for datasets with a large number of samples.\n",
    "\n",
    "* **3. Categorical Features:** It can handle both numerical and categorical features directly without the need for preprocessing such as one-hot encoding.\n",
    "\n",
    "* **4.Regularization:** The algorithm supports L1 (Lasso) and L2 (Ridge) regularization, helping to control overfitting.\n",
    "\n",
    "* **5. Early Stopping:** HistGradientBoostingClassifier supports early stopping based on the validation score to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeddf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the HistGradientBoostingClassifier\n",
    "histgbclassifier = HistGradientBoostingClassifier(random_state=42, max_iter=58, interaction_cst='pairwise', l2_regularization=2,\n",
    "                                                 learning_rate=0.5, max_bins=254, max_depth=4, max_leaf_nodes=16, \n",
    "                                                 min_samples_leaf=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894d7946",
   "metadata": {},
   "outputs": [],
   "source": [
    "histgbclassifier.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f1b27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = histgbclassifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc935cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74639c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "histgb_param_grid = {\n",
    "    'learning_rate' : [0.48,0.5,0.52],\n",
    "    'max_iter' : [58,60,63],\n",
    "    'max_leaf_nodes' : [15,16,17,],\n",
    "    'max_depth' : [3,4,5],\n",
    "    'min_samples_leaf' : [29,30,31],\n",
    "    'l2_regularization' : [1,2,3],\n",
    "    'max_bins' : [252,253,254],\n",
    "    'interaction_cst' : ['pairwise','no_interactions'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ddd60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "histgb_grid = GridSearchCV(estimator=histgbclassifier, param_grid=histgb_param_grid, cv=5, verbose=3, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f07878",
   "metadata": {},
   "outputs": [],
   "source": [
    "histgb_grid.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702a0a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "histgb_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6e37b6",
   "metadata": {},
   "source": [
    "# Gaussian Process Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2723ef3b",
   "metadata": {},
   "source": [
    "* **Gaussian Process Classification(GPC)** is a **probabilistic model** that extends **Gaussian Processes(GP)** to the classification setting. Gaussian Processes are a powerful tool in machine learning for modeling complex relationships, and GPC adapts this framework to handle classification problems.\n",
    "\n",
    "* In GPC, the goal is to model the probability distribution over functions that map input features to class labels. Here's a brief overview of how Gaussian Process Classification works:\n",
    "\n",
    "* **1. Gaussian Process:** A Gaussian Process is a collection of random variables, any finite number of which have a joint Gaussian distribution. In the context of GPC, the Gaussian Process models the distribution over functions.\n",
    "\n",
    "* **2. Binary Classification:** GPC is often used for binary classification problems, where there are two classes (positive and negative).\n",
    "\n",
    "* **3. Probabilistic Prediction:** Instead of providing a deterministic prediction, GPC provides a probability distribution over possible class labels for a given input.\n",
    "\n",
    "* **4. Kernel Function:** The choice of kernel function plays a crucial role in defining the covariance structure of the Gaussian Process. Common kernels include the Radial Basis Function (RBF) kernel and the Mat√©rn kernel.\n",
    "\n",
    "* **5. Hyperparameters:** GPC involves hyperparameters such as the length scale and amplitude of the kernel, which need to be tuned during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12115ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the kernel (RBF kernel in this case)\n",
    "kernel = 1.0 * RBF(length_scale=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45f7d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the GaussianProcessClassifier\n",
    "gpc = GaussianProcessClassifier(kernel=kernel, random_state=42,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692d9bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the training data\n",
    "gpc.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c90e1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "predictions = gpc.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534d6ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e406c3bd",
   "metadata": {},
   "source": [
    "# MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc56274",
   "metadata": {},
   "source": [
    "* **MLPClassifier** stands for **Multi-Layer Perceptron Classifier**. It is a type of artificial neural network used for classification tasks. The term \"multi-layer perceptron\" refers to the structure of the network, which consists of multiple layers of interconnected nodes or neurons.\n",
    "\n",
    "**1. Neural Network Architecture:** MLPClassifier is based on a feedforward neural network with one or more hidden layers. Each layer contains a certain number of neurons, and the neurons are connected with weights.\n",
    "\n",
    "**2. Activation Function:** Neurons in each layer use an activation function to introduce non-linearity into the model. Common activation functions include the logistic (sigmoid) function, hyperbolic tangent function (tanh), or rectified linear unit (ReLU).\n",
    "\n",
    "**3. Training:** The model is trained using backpropagation and gradient descent. The weights of the connections are adjusted during training to minimize the difference between the predicted outputs and the true labels.\n",
    "\n",
    "**4. Loss Function:** The optimization process involves minimizing a loss function, which measures the difference between the predicted and actual values. For classification tasks, the cross-entropy loss is often used.\n",
    "\n",
    "**5. Hyperparameters:** Important hyperparameters include the number of hidden layers, the number of neurons in each hidden layer, the learning rate, activation functions, and regularization parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa805303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the MLPClassifier\n",
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=(50,), max_iter=110, random_state=42, activation='identity', alpha=7.5e-07,\n",
    "                              batch_size=110, solver='lbfgs', max_fun=15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545cd495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the training data\n",
    "mlp_classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b910e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "predictions = mlp_classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822dcfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ccfc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_param_grid = {\n",
    "    'hidden_layer_sizes' : [(45,),(50,),(60,)],\n",
    "    'activation' : ['identity', 'tanh', 'logistic', 'relu'],\n",
    "    'solver' : ['lbfgs','sgd','adam'],\n",
    "    'alpha' : [0.00000075,0.0000125,0.00002,],\n",
    "    'batch_size' : [110,120,125],\n",
    "    'max_iter' : [110,120,125]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d553b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_grid = GridSearchCV(estimator=mlp_classifier, param_grid=mlp_param_grid, cv=5, verbose=3, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26204d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_grid.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111e2d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ff5892",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762d8e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=42,C=0.4, max_iter=40, penalty='l1', solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93268f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff85176e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lr.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ea3f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a4e85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_param_grid = {\n",
    "    'penalty' : ['l2','l1','elasticnet' ],\n",
    "    'C' : [0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45],\n",
    "    'max_iter' : [20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40],\n",
    "    'solver' : ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15c8340",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_best = GridSearchCV(estimator=lr, param_grid=lr_param_grid, cv=5, verbose=3, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71944d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_best.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102e7800",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_best.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5512db",
   "metadata": {},
   "source": [
    "# Categorical Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5010f81",
   "metadata": {},
   "source": [
    "* **Categorical Naive Bayes** is a variant of the Naive Bayes algorithm specifically designed for datasets with categorical features. In standard Naive Bayes, the algorithm assumes that features are continuous and follow a particular distribution, usually Gaussian. However, when dealing with categorical data (features that can take on discrete values and have no inherent order), the assumptions of the standard Naive Bayes may not hold.\n",
    "\n",
    "* Categorical Naive Bayes models the likelihood of categorical features using a categorical distribution. This makes it suitable for datasets where features are discrete and categorical, such as colors, types of products, or categorical labels. It is an extension of the Naive Bayes algorithm that caters to the characteristics of categorical data.\n",
    "\n",
    "**1. Categorical Distribution:** The likelihood of each feature is modeled using a categorical distribution. This distribution is appropriate for discrete data with a finite number of categories.\n",
    "\n",
    "**2. Independence Assumption:** Like other Naive Bayes variants, Categorical Naive Bayes assumes independence between features given the class label. This simplifying assumption allows for efficient training and prediction.\n",
    "\n",
    "**3. Smoothing:** To handle cases where certain category combinations might not be present in the training data, smoothing techniques, such as Laplace smoothing, are often applied.\n",
    "\n",
    "**4. Maximum Likelihood Estimation:** The parameters of the categorical distribution (probabilities for each category) are estimated from the training data using Maximum Likelihood Estimation (MLE).\n",
    "\n",
    "**5. Predictive Modeling:** Given a set of categorical features, Categorical Naive Bayes calculates the probability of each class and predicts the class with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4279aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CategoricalNB model\n",
    "cnb = CategoricalNB(alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e74c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the training data\n",
    "cnb.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96e0480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "predictions = cnb.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604e787e",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9dd287",
   "metadata": {},
   "source": [
    "# Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b8e494",
   "metadata": {},
   "source": [
    "* **Gaussian Naive Bayes** is a variant of the Naive Bayes algorithm that is well-suited for datasets where the features are continuous and assumed to follow a Gaussian (normal) distribution. It is part of the Naive Bayes family of classifiers, which are based on Bayes' theorem and the assumption of feature independence given the class label.\n",
    "\n",
    "**1. Independence Assumption:** Like all Naive Bayes classifiers, Gaussian Naive Bayes assumes that the features are conditionally independent given the class label. This is a simplifying assumption that facilitates efficient training and prediction.\n",
    "\n",
    "**2. Continuous Features:** Gaussian Naive Bayes is designed for datasets with continuous features. It models the distribution of each feature in each class as a Gaussian distribution (bell curve).\n",
    "\n",
    "**3. Parameter Estimation:** The parameters of the Gaussian distribution, namely the mean and standard deviation, are estimated for each feature in each class from the training data. This is typically done using Maximum Likelihood Estimation (MLE).\n",
    "\n",
    "**4. Bayes' Theorem:** The classification decision is made based on Bayes' theorem. Given an observation with a set of feature values, the model calculates the posterior probability of each class and selects the class with the highest probability as the predicted class.\n",
    "\n",
    "**5. Decision Boundary:** In Gaussian Naive Bayes, the decision boundaries are quadratic, and the algorithm assumes that the covariance of the features within each class is diagonal (features are uncorrelated).\n",
    "\n",
    "**6. Scalability:** Gaussian Naive Bayes is computationally efficient and often performs well on high-dimensional datasets. However, it may not capture complex relationships between features as accurately as more complex models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa60451",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB(var_smoothing=1.034e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfc66c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b3afb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = gnb.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9278fdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10df3981",
   "metadata": {},
   "source": [
    "# Complement Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d647b7a",
   "metadata": {},
   "source": [
    "* **Complement Naive Bayes(CNB)** is a variant of the Naive Bayes algorithm that is designed to address the issue of imbalanced datasets. Like other Naive Bayes classifiers, CNB is based on Bayes' theorem and assumes that features are conditionally independent given the class label. However, it introduces a complementary approach to the traditional Naive Bayes formulation.\n",
    "\n",
    "**1. Imbalanced Datasets:** CNB is particularly useful for imbalanced datasets where some classes are underrepresented compared to others. Traditional Naive Bayes models, including Multinomial and Gaussian variants, assume that features are indicative of the presence of a class. In imbalanced datasets, this assumption can lead to biased predictions.\n",
    "\n",
    "**2. Complementary Approach:** CNB takes a complementary approach to feature probabilities. Instead of focusing on the probability of features occurring given a class, it calculates the probability of features occurring given the absence of a class. This is especially useful for rare classes where the majority of instances belong to other classes.\n",
    "\n",
    "**3. Class Probability Calculation:** CNB calculates the class probability by considering the likelihood of observing features in the complement set (features not belonging to the target class). The class with the lowest complement probability is then predicted.\n",
    "\n",
    "**4. Hyperparameter Œ±:** CNB introduces a smoothing hyperparameter (Œ±) to handle cases where certain features are absent from the training data. This helps prevent zero probabilities, especially when dealing with sparse datasets.\n",
    "\n",
    "**5. Scalability:** CNB is computationally efficient and tends to scale well with large datasets. It is particularly useful when dealing with text classification tasks where the number of possible features (e.g., words in a vocabulary) can be large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4cb9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnb = ComplementNB(alpha=0.08,norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ce3483",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnb.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69e0f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = cnb.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881a24a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329e7572",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnb_param_grid = {\n",
    "    'alpha' : [0.08,0.081,0.082,0.083,0.084,0.085,0.086,0.087,0.088,0.089,0.09,0.0901,0.0902,0.0903,0.0904,0.0905,0.0906,0.0907,0.0908,0.909],\n",
    "    'norm' : [True,False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23428085",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnb_best = GridSearchCV(estimator=cnb, param_grid=cnb_param_grid, cv=5, verbose=3, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba829214",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnb_best.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241040ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnb_best.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b589627f",
   "metadata": {},
   "source": [
    "# Neural Networks with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "478797df",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential([\n\u001b[1;32m----> 2\u001b[0m             Dense(\u001b[38;5;241m500\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(\u001b[43mx_train\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],)),\n\u001b[0;32m      3\u001b[0m             Dropout(\u001b[38;5;241m0.2\u001b[39m),\n\u001b[0;32m      4\u001b[0m     \n\u001b[0;32m      5\u001b[0m             Dense(\u001b[38;5;241m500\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m      6\u001b[0m             Dropout(\u001b[38;5;241m0.2\u001b[39m),\n\u001b[0;32m      7\u001b[0m     \n\u001b[0;32m      8\u001b[0m             Dense(\u001b[38;5;241m100\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m      9\u001b[0m             Dropout(\u001b[38;5;241m0.2\u001b[39m),\n\u001b[0;32m     10\u001b[0m     \n\u001b[0;32m     11\u001b[0m             Dense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Output layer for binary classification\u001b[39;00m\n\u001b[0;32m     12\u001b[0m ])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "            Dense(500, activation='relu', input_shape=(x_train.shape[1],)),\n",
    "            Dropout(0.2),\n",
    "    \n",
    "            Dense(500, activation='relu'),\n",
    "            Dropout(0.2),\n",
    "    \n",
    "            Dense(100, activation='relu'),\n",
    "            Dropout(0.2),\n",
    "    \n",
    "            Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2f518e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7268914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define EarlyStopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor the validation loss\n",
    "    patience=5,           # Number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True)  # Restore model weights from the epoch with the best value of monitored quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44406333",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(x_train, y_train, epochs=50, batch_size=4, validation_split=0.1, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48147c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f'Test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c436d6",
   "metadata": {},
   "source": [
    "# Neural Network with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186b47e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['income'],axis=1).values\n",
    "Y = df['income'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25b39f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921ce03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)  # Ensure y is a column vector\n",
    "x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4dcf60",
   "metadata": {},
   "source": [
    "* **torch.tensor(x_train, dtype=torch.float32):** Converts the feature matrices x_train and x_test to PyTorch tensors. This ensures that the data type is set to float32, which is a common data type for neural network computations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9bb0ac",
   "metadata": {},
   "source": [
    "* **torch.tensor(y_train, dtype=torch.float32):** Converts the target vectors y_train and y_test to PyTorch tensors with data type float32.\n",
    "\n",
    "* **view(-1, 1):** Ensures that the target vectors are column vectors. The -1 is used to infer the size along that dimension, and 1 specifies the desired size along the second dimension (column vector)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5651e97f",
   "metadata": {},
   "source": [
    "* PyTorch models are designed to work with PyTorch tensors. When you create a neural network using PyTorch, the input to the model, as well as the model parameters and output, are all represented as PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720e309b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771a6b8d",
   "metadata": {},
   "source": [
    "* **TensorDataset** is used to create PyTorch datasets (train_dataset and test_dataset) from the training and testing data.\n",
    "\n",
    "* The original data is split into features (X) and target variable (y).\n",
    "\n",
    "* TensorDataset is used to combine these features and labels into a PyTorch dataset (train_dataset and test_dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b21a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620fba58",
   "metadata": {},
   "source": [
    "* The **DataLoader** in PyTorch is a utility that helps efficiently load and iterate over a dataset during the training or evaluation of machine learning models.\n",
    "\n",
    "* DataLoader divides the dataset into batches, which are smaller subsets of the data. This is beneficial for training large models that might not fit into memory.\n",
    "\n",
    "* Optionally, DataLoader can shuffle the data during each epoch (or iteration), ensuring that the model sees different samples in each batch. Shuffling helps prevent the model from memorizing the order of the data and promotes better generalization.\n",
    "\n",
    "* DataLoader can load batches in parallel using multiple workers. This can significantly speed up the data loading process, especially when dealing with large datasets.\n",
    "\n",
    "* DataLoader is iterable, allowing you to use a for loop to iterate over batches of data. This is essential for feeding batches into your model during training.\n",
    "\n",
    "* DataLoader integrates seamlessly with PyTorch models. You can use it directly within the training loop to load batches and pass them to your model for training or evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed25b91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network class\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 100)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(100, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceda33e3",
   "metadata": {},
   "source": [
    "**1. Import PyTorch's Neural Network Module:**\n",
    "* **import torch.nn as nn:** Imports the nn module from PyTorch, which contains classes and functions for building neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722808f5",
   "metadata": {},
   "source": [
    "**2. Define the Neural Network Class:**\n",
    "* **class SimpleNN(nn.Module):**: Defines a new class named SimpleNN that inherits from nn.Module, the base class for all PyTorch neural network modules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2279d9ce",
   "metadata": {},
   "source": [
    "**3. Initialize the Neural Network:**\n",
    "* **def __init__(self, input_size):**: Constructor method for the SimpleNN class. Takes input_size as a parameter, representing the number of input features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ce4ea4",
   "metadata": {},
   "source": [
    "**4. Call the Superclass Constructor:**\n",
    "* **super(SimpleNN, self).__init__()**: Calls the constructor of the superclass (nn.Module) to initialize the base class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab42f21",
   "metadata": {},
   "source": [
    "**5. Define the First Fully Connected Layer (fc1):**\n",
    "* **self.fc1 = nn.Linear(input_size, 64)**: Defines the first fully connected layer (fc1) with input_size input features and 64 output features. This layer represents a linear transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f972475",
   "metadata": {},
   "source": [
    "**6. Define the ReLU Activation (relu):**\n",
    "* **self.relu = nn.ReLU()**: Defines the Rectified Linear Unit (ReLU) activation function (relu). ReLU introduces non-linearity into the network by applying an element-wise rectified linear function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6c6cbc",
   "metadata": {},
   "source": [
    "**7. Define the Second Fully Connected Layer (fc2):**\n",
    "* **self.fc2 = nn.Linear(64, 1)**: Defines the second fully connected layer (fc2) with 64 input features (from the previous layer) and 1 output feature. This layer represents another linear transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ca441e",
   "metadata": {},
   "source": [
    "**8. Define the Sigmoid Activation (sigmoid):**\n",
    "* **self.sigmoid = nn.Sigmoid()**: Defines the Sigmoid activation function (sigmoid). Sigmoid is often used in binary classification problems to squash the output between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a714579",
   "metadata": {},
   "source": [
    "**9. Define the Forward Pass Method:**\n",
    "* **def forward(self, x):**: Defines the forward pass method for the neural network. This method specifies how input data x is processed through the network layers.\n",
    "\n",
    "* The forward pass involves applying the linear transformations, activation functions, and returning the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b22f633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "input_size = x_train.shape[1]\n",
    "model = SimpleNN(input_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b49937e",
   "metadata": {},
   "source": [
    "* Passing the data into the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77ff94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e596bf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Convert probabilities to binary predictions (0 or 1)\n",
    "            predictions = (outputs > 0.5).float()\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        accuracy = correct / total\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Test Loss: {test_loss/len(test_loader)}, Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751b44ce",
   "metadata": {},
   "source": [
    "* We define a simple neural network class (SimpleNN) with two fully connected layers and ReLU activation between them.\n",
    "\n",
    "* The forward method defines the forward pass of the network.\n",
    "\n",
    "* We use the Binary Cross Entropy Loss (BCELoss) as the loss function and Adam optimizer.\n",
    "\n",
    "* We train the model using a loop over multiple epochs, iterating through batches of data.\n",
    "\n",
    "* The model is evaluated on the test set after each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e720df5",
   "metadata": {},
   "source": [
    "**1. Train the model loop:**\n",
    "* **num_epochs = 50**: Specifies the number of times the entire training dataset will be passed forward and backward through the neural network during training.\n",
    "\n",
    "* **for epoch in range(num_epochs):**: Iterates over each epoch.\n",
    "\n",
    "* **model.train()**: Sets the model to training mode. This is necessary if the model contains layers like dropout or batch normalization that behave differently during training and evaluation.\n",
    "\n",
    "* **for inputs, labels in train_loader:**: Iterates over batches of training data loaded by the train_loader.\n",
    "\n",
    "* **optimizer.zero_grad()**: Clears the gradients of all optimized tensors before performing backpropagation.\n",
    "\n",
    "* **outputs = model(inputs)**: Performs a forward pass to get model predictions for the input data.\n",
    "\n",
    "* **loss = criterion(outputs, labels)**: Computes the loss between the model predictions (outputs) and the actual labels (labels).\n",
    "\n",
    "* **loss.backward()**: Computes the gradient of the loss with respect to the model parameters.\n",
    "\n",
    "* **optimizer.step()**: Updates the model parameters based on the computed gradients using the specified optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0b5e50",
   "metadata": {},
   "source": [
    "**2. Evaluate the model on the test set loop:**\n",
    "* **model.eval()**: Sets the model to evaluation mode. This is necessary if the model contains layers like dropout or batch normalization that behave differently during training and evaluation.\n",
    "\n",
    "* **with torch.no_grad():**: Disables gradient computation during evaluation to save memory.\n",
    "\n",
    "* **for inputs, labels in test_loader:**: Iterates over batches of test data loaded by the test_loader.\n",
    "\n",
    "* **outputs = model(inputs)**: Performs a forward pass to get model predictions for the input data.\n",
    "\n",
    "* **loss = criterion(outputs, labels)**: Computes the loss between the model predictions and the actual labels.\n",
    "\n",
    "* **test_loss += loss.item()**: Accumulates the test loss.\n",
    "\n",
    "* **predictions = (outputs > 0.5).float()**: Converts model output probabilities to binary predictions (0 or 1) using a threshold of 0.5.\n",
    "\n",
    "* **correct += (predictions == labels).sum().item()**: Counts the number of correct predictions.\n",
    "\n",
    "* **total += labels.size(0)**: Accumulates the total number of samples in the test set.\n",
    "\n",
    "* **accuracy = correct / total**: Computes the test accuracy.\n",
    "\n",
    "* **print(f'Epoch {epoch+1}/{num_epochs}, Test Loss: {test_loss/len(test_loader)}, Test Accuracy: {accuracy}')**: Prints the test loss and accuracy for the current epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a048ce",
   "metadata": {},
   "source": [
    "# Neural Networks created using Neurolab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8ea09d",
   "metadata": {},
   "source": [
    "* **NeuroLab** is a Python library for developing and training artificial neural networks. It provides a simple and convenient interface for constructing various types of neural networks, including feedforward and recurrent networks. NeuroLab supports supervised learning through backpropagation and unsupervised learning through competitive learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e19744b",
   "metadata": {},
   "source": [
    "* **1. Ease of Use**: NeuroLab is designed to be user-friendly and accessible to beginners. It provides a high-level interface for creating and training neural networks, allowing users to focus on model design and experimentation.\n",
    "\n",
    "* **2. Flexibility**: The library supports the construction of various types of neural network architectures, including multilayer perceptrons (MLPs), radial basis function (RBF) networks, and self-organizing maps (SOMs). This flexibility allows users to choose the most suitable architecture for their specific tasks.\n",
    "\n",
    "* **3. Integration with NumPy**: NeuroLab integrates with the popular numerical computing library NumPy, making it easy to work with numerical data and perform computations efficiently.\n",
    "\n",
    "* **4. Supervised and Unsupervised Learning**: NeuroLab supports both supervised learning, where the network is trained on input-output pairs, and unsupervised learning, where the network discovers patterns in the input data without explicit output labels.\n",
    "\n",
    "* **5. Visualization Tools**: The library includes tools for visualizing neural network structures and training progress, helping users understand and analyze their models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c933d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7397d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable\n",
    "X = df.drop('income', axis=1).values\n",
    "y = df['income'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1043e09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1795060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feedforward neural network\n",
    "net = nl.net.newff([[min_val, max_val] for min_val, max_val in zip(np.min(X_train, axis=0), np.max(X_train, axis=0))], [10, 1], transf=[nl.trans.LogSig(), nl.trans.LogSig()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4927fca",
   "metadata": {},
   "source": [
    "* **1. nl.net.newff**: This is a function in NeuroLab used to create a new feedforward neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d90106",
   "metadata": {},
   "source": [
    "**2. [[min_val, max_val] for min_val, max_val in zip(np.min(X_train, axis=0), np.max(X_train, axis=0))]**:\n",
    "* This part creates a list of input ranges for each feature in your dataset. It uses list comprehension to iterate over the minimum and maximum values of each feature in the training set (X_train).\n",
    "\n",
    "* **np.min(X_train, axis=0)** and np.max(X_train, axis=0) calculate the minimum and maximum values along each feature axis, respectively.\n",
    "\n",
    "* The resulting list is something like **[[min_feature1, max_feature1], [min_feature2, max_feature2], ..., [min_feature_n, max_feature_n]]**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c56da5d",
   "metadata": {},
   "source": [
    "**3.  [10, 1]**: This part specifies the number of neurons in each layer of the network. The first element [10] indicates that there are 10 neurons in the hidden layer, and the second element [1] indicates that there is 1 neuron in the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5021aba0",
   "metadata": {},
   "source": [
    "**4. transf=[nl.trans.LogSig(), nl.trans.LogSig()]:**\n",
    "* This part specifies the transfer functions for each layer of the network. The LogSig (logarithmic sigmoid) transfer function is commonly used in the hidden and output layers of neural networks for binary classification tasks.\n",
    "\n",
    "* The first element **nl.trans.LogSig()** indicates the **transfer function for the hidden layer**, and the **second element nl.trans.LogSig() indicates the transfer function for the output layer**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bb2b29",
   "metadata": {},
   "source": [
    "**5. net = nl.net.newff(...):** Combining all the specifications, this line creates a new feedforward neural network (net) with the specified input ranges, number of neurons in each layer, and transfer functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da7df780",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\Research\\lib\\site-packages\\neurolab\\trans.py:107: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1; Error: 2963.3352962944937;\n",
      "Epoch: 2; Error: 2256.0190650885074;\n",
      "Epoch: 3; Error: 2256.0190642600587;\n",
      "Epoch: 4; Error: 2256.0190642066727;\n",
      "Epoch: 5; Error: 2256.0190641966155;\n",
      "Epoch: 6; Error: 2256.01906419661;\n"
     ]
    }
   ],
   "source": [
    "# Train the network\n",
    "error = net.train(X_train, y_train.reshape(-1, 1), epochs=200, show=1, goal=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b33997",
   "metadata": {},
   "source": [
    "* **show=10**: This parameter controls how often the training progress is displayed during the training process. In this case, it means that the training progress will be displayed every 10 epochs. The output will show information such as the current epoch, the error, and potentially other relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83316335",
   "metadata": {},
   "source": [
    "* **goal=0.01**: This parameter sets the training goal, indicating the desired level of training error. The training process will continue until the error falls below this specified goal or the maximum number of epochs is reached. In this example, the goal is set to 0.01, suggesting that the training will continue until the error on the training data is below 0.01 or until 100 epochs are completed (as specified by epochs=100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84af11f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "predictions = net.sim(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a78eb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.14%\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = np.mean((predictions > 0.5).astype(int) == y_test.reshape(-1, 1))\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2b1ba3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
